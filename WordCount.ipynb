{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial para executar a aplicação wordcount mapreduce na linha de comandos do Hadoop na VM Cloudera. Esta aplicação conta palavras em uma base de dados do twitter. Base de dados utilizada: Mensagens da rede social Twitter sobre os temas: Big Data, data visualization, data privacy e Internet of Things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 1: Submeter base de dados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Comando que cria no hdfs o direotorio onde será armazenado o arquivo base_tw.txt que será consumido pela aplicação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs dfs -mkdir /user/cloudera/bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Comando para submeter o arquivo para o diretório:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs dfs -put ~/Documents/base_tw.txt /user/cloudera/bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Comando para verificar se o arquivo foi gravado no hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2: Execução do WordCount jar file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount bases/base_tw.txt saida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar: executa o camando /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar: mapreduce jar file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordcount: Importação de classes da biblioteca Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bases/base_tw.txt: diretório e arquivo que será consumido pela aplicação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saida: diretório que será criado no momento da execução e que conterá o arquivo resultado da aplicação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez finalizado a execução do comando será criado o arquivo de nome part-r-00000 dentro do diretorio saida do hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 3: Visualizar resultado de execução do WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comando para listar o conteúdo do diretório saida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs dfs -ls saida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comando para visualizar o conteudo do arquivo part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part-r-00000 hdfs dfs -cat saida/part-r-00000"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "SciJava",
   "language": "groovy",
   "name": "scijava"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": "",
   "mimetype": "",
   "name": "scijava",
   "nbconverter_exporter": "",
   "pygments_lexer": "groovy",
   "version": "1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
